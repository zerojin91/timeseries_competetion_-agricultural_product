# -*- coding: utf-8 -*-
"""dl_inference.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tfFyHmb9xy4MWr9XDcOff0NxEL0sDCmO
"""

import torch
import pandas as pd
import urllib.request
import json
import datetime
from datetime import timedelta
from tqdm.notebook import tqdm
import pandas as pd
import numpy as np
from glob import glob

import multiprocessing
import pickle
import easydict
import random
import os
import math
import torch
from torch import nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader

def seed_everything(seed: int = 42):
    random.seed(seed)
    np.random.seed(seed)
    os.environ["PYTHONHASHSEED"] = str(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)  # type: ignore
    torch.backends.cudnn.deterministic = True  # type: ignore
    torch.backends.cudnn.benchmark = True  # type: ignore


def fe(df, train=True):
    # 기존 전처리
    df_ = df.copy()
    start_day = df_['date'].values[0]
    end_day = df_['date'].values[-1]
    n_df = pd.DataFrame([str(i).split(" ")[0] for i in pd.date_range(start_day, end_day, freq='D')])
    n_df.columns = ["date"]
    df_ = pd.merge(n_df, df_, on="date",how="left")
    df_ = df_.replace(0, np.nan)
    df_ = df_.fillna(method='ffill')
    if train:
        df_ = df_[~df_['샤인마스캇_가격(원/kg)'].isnull()].reset_index(drop=True)
    df_.iloc[:,1:] = df_.iloc[:,1:].ewm(alpha=0.3).mean()

    # 변수 추가
    dates = pd.to_datetime(df_['date'].values)
    df_['day_of_week'] = dates.dayofweek.astype('str')
    df_['day_of_month'] = dates.day.astype('str')
    df_['week_of_year'] = pd.Int64Index(dates.isocalendar().week).astype('str')
    df_['month'] = dates.month.astype('str')
    df_['year'] = dates.year.astype('str')

    df_.drop(["요일"], axis=1, inplace=True)
    
    return df_

def get_sinusoid_encoding_table(n_seq, hidn):

    def cal_angle(position, i_hidn):
        return position / np.power(10000, 2 * (i_hidn // 2) / hidn)

    def get_posi_angle_vec(position):
        return [cal_angle(position, i_hidn) for i_hidn in range(hidn)]

    sinusoid_table = np.array([get_posi_angle_vec(i_seq) for i_seq in range(n_seq)])
    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # even index sin
    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # odd index cos

    return sinusoid_table


def get_attn_decoder_mask(seq):
    batch, window_size, d_hidn = seq.size()
    subsequent_mask = torch.ones((batch,window_size,window_size), device=seq.device)
    subsequent_mask = subsequent_mask.triu(diagonal=1)
    return subsequent_mask


class scaleddotproductattention(nn.Module):
    def __init__(self, args):
        super().__init__()
        self.args = args
        self.dropout = nn.Dropout(self.args.dropout)
        self.scale = 1 / (self.args.d_head ** 0.5)

    def forward(self, q, k, v, attn_mask=False):
        scores = torch.matmul(q, k.transpose(-1, -2))
        scores = scores.mul_(self.scale)

        if attn_mask is not False:
            scores.masked_fill_(attn_mask, -1e9)
            attn_prob = nn.Softmax(dim=-1)(scores)
            attn_prob = self.dropout(attn_prob)
            context = torch.matmul(attn_prob, v)
        else:
            attn_prob = nn.Softmax(dim=-1)(scores)
            attn_prob = self.dropout(attn_prob)
            context = torch.matmul(attn_prob, v)

        return context, attn_prob


class multiheadattention(nn.Module):
    def __init__(self, args):
        super().__init__()
        self.args = args
        self.W_Q = nn.Linear(self.args.d_hidn, self.args.n_head * self.args.d_head)
        self.W_K = nn.Linear(self.args.d_hidn, self.args.n_head * self.args.d_head)
        self.W_V = nn.Linear(self.args.d_hidn, self.args.n_head * self.args.d_head)
        self.scaled_dot_attn = scaleddotproductattention(self.args)
        self.linear = nn.Linear(self.args.n_head * self.args.d_head, self.args.d_hidn)
        self.dropout = nn.Dropout(self.args.dropout)

    def forward(self, q, k, v, attn_mask=False):
        batch_size = q.size(0)
        q_s = self.W_Q(q).view(batch_size, -1, self.args.n_head, self.args.d_head).transpose(1,2)
        k_s = self.W_K(k).view(batch_size, -1, self.args.n_head, self.args.d_head).transpose(1,2)
        v_s = self.W_V(v).view(batch_size, -1, self.args.n_head, self.args.d_head).transpose(1,2)

        if attn_mask is not False:
            attn_mask = attn_mask.unsqueeze(1).repeat(1, self.args.n_head, 1, 1)
            context, attn_prob = self.scaled_dot_attn(q_s, k_s, v_s, attn_mask)
        else:
            context, attn_prob = self.scaled_dot_attn(q_s, k_s, v_s)
        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.args.n_head * self.args.d_head)

        output = self.linear(context)
        output = self.dropout(output)

        return output, attn_prob


class poswisefeedforwardnet(nn.Module):
    def __init__(self, args):
        super().__init__()
        self.args = args
        self.conv1 = nn.Conv1d(in_channels=self.args.d_hidn, out_channels=self.args.d_ff, kernel_size=1)
        self.conv2 = nn.Conv1d(in_channels=self.args.d_ff, out_channels=self.args.d_hidn, kernel_size=1)
        self.active = F.gelu
        self.dropout = nn.Dropout(self.args.dropout)

    def forward(self, inputs):
        output = self.conv1(inputs.transpose(1, 2).contiguous())
        output = self.active(output)
        output = self.conv2(output).transpose(1, 2).contiguous()
        output = self.dropout(output)

        return output


class encoderlayer(nn.Module):
    def __init__(self, args):
        super().__init__()
        self.args = args
        self.self_attn = multiheadattention(self.args)
        self.pos_ffn = poswisefeedforwardnet(self.args)
        self.layer_norm1 = nn.LayerNorm(self.args.d_hidn, eps=self.args.layer_norm_epsilon)
        self.layer_norm2 = nn.LayerNorm(self.args.d_hidn, eps=self.args.layer_norm_epsilon)

    def forward(self, inputs):
        att_outputs, attn_prob = self.self_attn(inputs, inputs, inputs)
        att_outputs = self.layer_norm1(att_outputs + inputs)

        ffn_outputs = self.pos_ffn(att_outputs)
        ffn_outputs = self.layer_norm2(ffn_outputs + att_outputs)

        return ffn_outputs, attn_prob


class encoder(nn.Module):
    def __init__(self, args):
        super().__init__()
        self.args = args
        self.enc_emb = nn.Linear(in_features=self.args.window_size, out_features=self.args.d_hidn, bias=False)
        nusoid_table = torch.FloatTensor(get_sinusoid_encoding_table(self.args.e_features , self.args.d_hidn))
        self.pos_emb = nn.Embedding.from_pretrained(nusoid_table, freeze=True)
        self.layers = nn.ModuleList([encoderlayer(self.args) for _ in range(self.args.n_layer)])
        self.enc_attn_probs = None

    def forward(self, inputs):
        self.enc_attn_probs = []
        positions = torch.arange(inputs.size(2), device=inputs.device).expand(inputs.size(0), inputs.size(2)).contiguous()
        outputs = self.enc_emb(inputs.transpose(2,1).contiguous()) + self.pos_emb(positions)
        for layer in self.layers:
            outputs, enc_attn_prob = layer(outputs)
            self.enc_attn_probs.append(enc_attn_prob)

        return outputs

class decoderlayer(nn.Module):
    def __init__(self, args):
        super().__init__()
        self.args = args
        self.self_attn = multiheadattention(self.args)
        self.dec_enc_attn = multiheadattention(self.args)
        self.pos_ffn = poswisefeedforwardnet(self.args)
        self.layer_norm1 = nn.LayerNorm(self.args.d_hidn, eps=self.args.layer_norm_epsilon)
        self.layer_norm2 = nn.LayerNorm(self.args.d_hidn, eps=self.args.layer_norm_epsilon)
        self.layer_norm3 = nn.LayerNorm(self.args.d_hidn, eps=self.args.layer_norm_epsilon)

    def forward(self, dec_inputs, enc_outputs, attn_mask):
        self_att_outputs, dec_attn_prob = self.self_attn(dec_inputs, dec_inputs, dec_inputs, attn_mask)
        self_att_outputs = self.layer_norm1(self_att_outputs + dec_inputs)
        
        dec_enc_att_outputs, dec_enc_attn_prob = self.dec_enc_attn(self_att_outputs, enc_outputs, enc_outputs)
        dec_enc_att_outputs = self.layer_norm2(dec_enc_att_outputs + self_att_outputs)

        ffn_outputs = self.pos_ffn(dec_enc_att_outputs)
        ffn_outputs = self.layer_norm3(ffn_outputs + dec_enc_att_outputs)
    
        return ffn_outputs, dec_attn_prob, dec_enc_attn_prob


class decoder(nn.Module):
    def __init__(self, args):
        super().__init__()
        self.args = args
        self.dec_emb = nn.Linear(in_features=self.args.d_features, out_features=self.args.d_hidn, bias=False)
        sinusoid_table = torch.FloatTensor(get_sinusoid_encoding_table(self.args.window_size , self.args.d_hidn))
        self.pos_emb = nn.Embedding.from_pretrained(sinusoid_table, freeze=True)
        self.layers = nn.ModuleList([decoderlayer(self.args) for _ in range(self.args.n_layer)])
        self.dec_attn_probs = None
        self.dec_enc_attn_probs = None

    def forward(self, dec_inputs, enc_outputs):
        self.dec_attn_probs = []
        self.dec_enc_attn_probs = []
        positions = torch.arange(dec_inputs.size(1), device=dec_inputs.device).expand(dec_inputs.size(0), dec_inputs.size(1)).contiguous()
        dec_output = self.dec_emb(dec_inputs) + self.pos_emb(positions)

        attn_mask = torch.gt(get_attn_decoder_mask(dec_inputs),0)

        for layer in self.layers:
            dec_outputs, dec_attn_prob, dec_enc_attn_prob = layer(dec_output, enc_outputs, attn_mask)
            self.dec_attn_probs.append(dec_attn_prob)
            self.dec_enc_attn_probs.append(dec_enc_attn_prob)

        return dec_outputs


class TimeDistributed(nn.Module):
    def __init__(self, module):
        super(TimeDistributed, self).__init__()
        self.module = module

    def forward(self, x):

        if len(x.size()) <= 2:
            return self.module(x)

        x_reshape = x.contiguous().view(-1, x.size(-1))  # (samples * timesteps, input_size)
        y = self.module(x_reshape)

        if len(x.size()) == 3:
            y = y.contiguous().view(x.size(0), -1, y.size(-1))

        return y


class transformer(nn.Module):
    def __init__(self, args):
        super().__init__()
        self.args = args
        self.lstm = nn.LSTM(input_size=389, hidden_size=256, batch_first=True, bidirectional=True, num_layers=3) # known 420, no 391
        self.encoder = encoder(self.args)
        self.decoder = decoder(self.args)
        self.fc1 = TimeDistributed(nn.Linear(in_features=512, out_features=256))
        self.fc2 = TimeDistributed(nn.Linear(in_features=256, out_features=64))
        self.fc3 = TimeDistributed(nn.Linear(in_features=64, out_features=self.args.output_size))
        self.embedding_layer_day_of_week = nn.Embedding(num_embeddings = 7, embedding_dim = 2)
        self.embedding_layer_day_of_month = nn.Embedding(num_embeddings = 31, embedding_dim = 8)
        self.embedding_layer_week_of_year = nn.Embedding(num_embeddings = 53, embedding_dim = 14)
        self.embedding_layer_month = nn.Embedding(num_embeddings = 12, embedding_dim = 3)
        self.embedding_layer_year = nn.Embedding(num_embeddings = 6, embedding_dim = 2) # 기존 5개년 + 21년도(+1)
        self.embedding_layer_ty = nn.Embedding(num_embeddings = 22, embedding_dim = 6) # 기존 품목 21 + 1종목
        self.embeddings = nn.ModuleList([
                                         self.embedding_layer_day_of_week, 
                                         self.embedding_layer_day_of_month, 
                                         self.embedding_layer_week_of_year, 
                                         self.embedding_layer_month,
                                         self.embedding_layer_year,
                                         self.embedding_layer_ty,
                                         ])
        

    def forward(self, enc_numeric_inputs, enc_category_inputs, dec_inputs):
        emb_data_list = []
        for idx_ in range(enc_category_inputs.size()[2]):
            emb_data = self.embeddings[idx_](enc_category_inputs[:,:,idx_])
            emb_data_list.append(emb_data)
        
        # (1) deep
        enc_category_emb_inputs = torch.cat(emb_data_list, axis=-1)
        enc_inputs = torch.cat([enc_numeric_inputs, enc_category_emb_inputs], axis=-1)
        enc_outputs = self.encoder(enc_inputs)
        dec_outputs = self.decoder(dec_inputs, enc_outputs)

        # (2) wide
        enc_c_1 = F.one_hot(enc_category_inputs[:,:,0], num_classes=7)
        enc_c_2 = F.one_hot(enc_category_inputs[:,:,1], num_classes=31)
        enc_c_3 = F.one_hot(enc_category_inputs[:,:,2], num_classes=53)
        enc_c_4 = F.one_hot(enc_category_inputs[:,:,3], num_classes=12)
        enc_c_5 = F.one_hot(enc_category_inputs[:,:,4], num_classes=6)
        enc_c_6 = F.one_hot(enc_category_inputs[:,:,5], num_classes=22)
        enc_category_onehot_inputs = torch.cat([
                                                enc_numeric_inputs,
                                                enc_c_1, 
                                                enc_c_2, 
                                                enc_c_3, 
                                                enc_c_4, 
                                                enc_c_5, 
                                                enc_c_6,
                                                dec_inputs,
                                                ], axis=-1)

        # (3) concat
        deep_wide_inputs = torch.cat([dec_outputs, enc_category_onehot_inputs], axis=-1)
        
        # (4) output
        lstm_outs, (h_0, c_0) = self.lstm(deep_wide_inputs)    
        outputs = F.relu(self.fc1(lstm_outs[:,-1,:]))
        outputs = F.relu(self.fc2(outputs))
        outputs = self.fc3(outputs)

        return outputs

def converter(pred, ty_info, s_df):
    pred_ = pred.cpu().detach().numpy().copy()
    s_p_df = s_df[s_df['SCALE']=='PRICE'].reset_index(drop=True)
    for que in range(0, len(ty_info)):
        converter_max = s_p_df[s_p_df['TY']==str(ty_info[que])]['max'].values[0]
        converter_min = 0
        pred_[que,:] = (pred_[que,:] * converter_max) + converter_min
        
    return pred_

def my_custom_metric(pred, true, ty_info):
    pred = converter(pred, ty_info)
    true = converter(true, ty_info)
    pred = pred[:, [7, 14, 28]]
    true = true[:, [7, 14, 28]]
    target = np.where(true!=0)
    true = true[target]
    pred = pred[target]
    score = np.mean(np.abs((true-pred))/(true))
    
    return score

def SMAPE(y_pred, y_true):
    return torch.mean(torch.abs(y_true - y_pred) / (torch.abs(y_true) + torch.abs(y_pred))) 

def MAPE(y_pred, y_true):
  return torch.mean(torch.abs((y_true - y_pred)) / (torch.abs(y_true) + 1e-8))

def nmae(answer_df, submission_df):
    answer = answer_df.iloc[:,1:].to_numpy()
    submission = submission_df.iloc[:,1:].to_numpy()
    target_idx = np.where(answer!=0)
    
    true = answer[target_idx]
    pred = submission[target_idx]
    
    score = np.mean(np.abs(true-pred)/true)
    
    return score

def at_nmae(answer_df, submission_df):
    week_1_answer = answer_df.iloc[0::3]
    week_2_answer = answer_df.iloc[1::3]
    week_4_answer = answer_df.iloc[2::3]
    
    idx_col_nm = answer_df.columns[0]
    week_1_submission = submission_df[submission_df[idx_col_nm].isin(week_1_answer[idx_col_nm])]
    week_2_submission = submission_df[submission_df[idx_col_nm].isin(week_2_answer[idx_col_nm])]
    week_4_submission = submission_df[submission_df[idx_col_nm].isin(week_4_answer[idx_col_nm])]
    
    score1 = nmae(week_1_answer, week_1_submission)
    score2 = nmae(week_2_answer, week_2_submission)
    score4 = nmae(week_4_answer, week_4_submission)
    
    score = (score1+score2+score4)/3
    
    return score


def add_data(args):
    df_ = pd.read_csv(args.save_path+'all_df.csv')
    start_date = '2021-09-28'
    today = str(datetime.date.today().year)+str(datetime.date.today().month).zfill(2)+str(datetime.date.today().day).zfill(2)
    available_day = str(pd.to_datetime(today) - timedelta(days=1)).split(" ")[0].replace("-","")

    url = 'https://www.nongnet.or.kr/api/whlslDstrQr.do?sdate=' 
    data_list = []

    if start_date != today:
        print("데이터 수집 시작")
        for day in tqdm(list(pd.date_range(start_date, available_day, freq='D'))):
            day_t = str(day).split(" ")[0].replace("-","")
            response = urllib.request.urlopen(url+day_t).read()
            response = json.loads(response)
            data = pd.DataFrame(response['data'])

            if len(data) > 0 :
                data_list.append(data)
            else:
                continue
        
        target = [
          '배추', '무', '양파', '건고추', '마늘', '대파', '얼갈이배추', '양배추', 
          '깻잎', '시금치', '미나리', '당근', '파프리카', '새송이', '팽이버섯', 
          '토마토', '청상추', '백다다기', '애호박', '캠벨얼리', '샤인마스캇'
         ]

        df = pd.concat(data_list).reset_index(drop=True)
        target_df = df[df['KIND_NM'].isin(target) | df['PUM_NM'].isin(target)].reset_index(drop=True)
        target_df.loc[target_df['PUM_NM'].isin(target), "TY"] = target_df[target_df['PUM_NM'].isin(target)]['PUM_NM']
        target_df.loc[target_df['TY'].isnull() & target_df['KIND_NM'].isin(target), "TY"] = target_df[target_df['TY'].isnull() & target_df['KIND_NM'].isin(target)]['KIND_NM']
        target_set_df = target_df[target_df['TOT_QTY']>0].groupby(['SALEDATE', 'TY'])[['TOT_QTY','TOT_AMT']].sum()
        target_set_df = target_set_df.reset_index()
        target_set_df['PRICE'] = target_set_df['TOT_AMT']/target_set_df['TOT_QTY']
        price_df = target_set_df[['SALEDATE',"TY", "TOT_QTY", 'PRICE']]

        re_df = price_df.pivot_table(["TOT_QTY", "PRICE"], index="SALEDATE", columns=["TY"],aggfunc="max")
        re_df = re_df.reset_index()
        re_df.columns = ['date', "건고추_가격(원/kg)", "깻잎_가격(원/kg)", "당근_가격(원/kg)",
                        "대파_가격(원/kg)", "마늘_가격(원/kg)", "무_가격(원/kg)", "미나리_가격(원/kg)",
                        "배추_가격(원/kg)", "백다다기_가격(원/kg)", "새송이_가격(원/kg)", "샤인마스캇_가격(원/kg)",
                        "시금치_가격(원/kg)", "애호박_가격(원/kg)", "양배추_가격(원/kg)", "양파_가격(원/kg)", 
                        "얼갈이배추_가격(원/kg)", "청상추_가격(원/kg)", "캠벨얼리_가격(원/kg)", "토마토_가격(원/kg)",
                        "파프리카_가격(원/kg)", "팽이버섯_가격(원/kg)",
                        '건고추_거래량(kg)',"깻잎_거래량(kg)","당근_거래량(kg)","대파_거래량(kg)","마늘_거래량(kg)",
                        "무_거래량(kg)","미나리_거래량(kg)","배추_거래량(kg)","백다다기_거래량(kg)","새송이_거래량(kg)",
                        "샤인마스캇_거래량(kg)","시금치_거래량(kg)","애호박_거래량(kg)","양배추_거래량(kg)","양파_거래량(kg)",
                        "얼갈이배추_거래량(kg)","청상추_거래량(kg)","캠벨얼리_거래량(kg)","토마토_거래량(kg)","파프리카_거래량(kg)",
                        "팽이버섯_거래량(kg)"]
        re_df["요일"] = np.nan
        re_df = re_df[list(df_.columns)]
        re_df['date'] = re_df['date'].apply(lambda x:x[:4]+"-"+x[4:6]+"-"+x[6:8])

        data_set = pd.concat([df_, re_df], axis=0)
        data_set.reset_index(drop=True, inplace=True)
        print("데이터 수집 종료")
        return data_set
    else:
        return df_


def dl_inference(save_path):
    today = str(datetime.date.today().year) + "-" + str(datetime.date.today().month).zfill(2) + "-" + str(datetime.date.today().day).zfill(2)
    seed_everything(42)
    args = easydict.EasyDict({
                            'output_size' : 29,
                            'window_size' : 29,
                            'batch_size' : 512,
                            'lr' : 3e-5,
                            'e_features' : 36,
                            'd_features' : 1,
                            'd_hidn' : 256,
                            'n_head' : 4,
                            'd_head' : 64,
                            'dropout' : 0.1,
                            'd_ff' : 256,
                            'n_layer' : 3,
                            'dense_h' : 256,
                            'epochs' : 30,
                            'device' : 'cuda' if torch.cuda.is_available() else 'cpu',
                            'save_path' : save_path,
                            "layer_norm_epsilon": 1e-12,
                            })
    
    data = add_data(args)

    s_df = pd.read_csv(args.save_path+"s_df.csv")
    s_df['TY'] = s_df['TY'].apply(lambda x:str(x))
    submission = pd.read_csv(args.save_path+'sample_submission.csv')
    test_df = fe(data, train=False)
    test_df = test_df.iloc[-args.window_size:] 

    scale_col = ["QTY", "PRICE"]
    all_fe = ['date', 'day_of_week', 'day_of_month', 'week_of_year', 'month', 'year']
    group_idx = {}
    data_box = []
    for idx, i in enumerate(range(1, len(test_df.columns[1:-6])+1, 2)):
        data = test_df[test_df.columns[i:i+2]]
        data.columns = ['QTY', 'PRICE']
        data_f = test_df[all_fe]
        df = pd.concat([data, data_f], axis=1)
        df["TY"] = str(idx+1)
        group_idx[idx+1] = test_df.columns[i].split("_")[0]
        data_box.append(df)

    test_df_fe_melt = pd.concat(data_box).reset_index(drop=True)
    for que in s_df["TY"].unique():
        for scale in scale_col:
            t_max = s_df[(s_df['SCALE'] == scale) & (s_df["TY"]==que)]['max'].values[0]
            t_min = 0
            test_df_fe_melt.loc[test_df_fe_melt['TY']==str(int(que)+1), scale] = (test_df_fe_melt[test_df_fe_melt['TY']==str(int(que)+1)][scale].values - t_min) / t_max

    test_df_fe_melt['day_of_month'] = test_df_fe_melt['day_of_month'].apply(lambda x:str(int(x)-1))
    test_df_fe_melt['week_of_year'] = test_df_fe_melt['week_of_year'].apply(lambda x:str(int(x)-1))
    test_df_fe_melt['month'] = test_df_fe_melt['month'].apply(lambda x:str(int(x)-1))
    test_df_fe_melt['year'] = test_df_fe_melt['year'].replace({'2016':0,'2017':1,'2018':2,'2019':3,'2020':4,'2021':5})
    test_df_fe_melt['TY'] = test_df_fe_melt['TY'].apply(lambda x:str(int(x)-1))

    test_encoder_numeric_data = []
    test_encoder_category_data = []
    test_decoder_data = []

    for test_que in test_df_fe_melt['TY'].unique():
        t_df = test_df_fe_melt[test_df_fe_melt['TY'] == test_que].reset_index(drop=True)
        encoder_numeric_data = t_df.iloc[:,0:1].values
        encoder_category_data = t_df.iloc[:,3:].values.astype('int')
        decoder_data = t_df.iloc[:, 1].values
        test_encoder_numeric_data.append(encoder_numeric_data)
        test_encoder_category_data.append(encoder_category_data)
        test_decoder_data.append(decoder_data.reshape(args.window_size, 1))
        
    test_enc_n = torch.tensor(np.stack(test_encoder_numeric_data), dtype=torch.float32).to(args.device)
    test_enc_c = torch.tensor(np.stack(test_encoder_category_data), dtype=torch.int64).to(args.device)
    test_ty_info = test_enc_c.cpu().detach().numpy()[:,0,-1]
    test_dec = torch.tensor(np.stack(test_decoder_data), dtype=torch.float32).to(args.device)

    window_size_list = [str(args.window_size)]
    fold_list = ['1', '2', '3', '4']
    output_list = []
    for win_size in window_size_list:
        for fold in fold_list:
            print(f'wide_deep_model_{fold}fold_{win_size}win.pth',"모델 예측")
            model = transformer(args).to(args.device)
            model.load_state_dict(torch.load(args.save_path+"fin/" + f'wide_deep_model_{fold}fold_{win_size}win.pth')) 
            model.eval()
            with torch.no_grad():
                output = model(test_enc_n, test_enc_c, test_dec)
                output = converter(output, test_ty_info, s_df)
            output_list.append(output)
    
    outputs = (output_list[0]/4) + (output_list[1]/4) + (output_list[2]/4) + (output_list[3]/4) 

    idx = submission[submission['예측대상일자'].str.contains(today)].index
    for idx___, target in enumerate(list(test_ty_info)):
        t_name = group_idx[target+1]
        t_inverse_name = [col_name for col_name in submission.columns if col_name.startswith(t_name) > 0][0]
        submission.loc[idx, t_inverse_name] = outputs[idx___, [7,14,28]]

    submission.to_csv(args.save_path+f"output/dl_{today}.csv", index=False)

    return print("결과 파일이", args.save_path+f"output/dl_{today}.csv","에 저장됐습니다.")

